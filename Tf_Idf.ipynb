{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tf-Idf ",
      "provenance": [],
      "authorship_tag": "ABX9TyN5BjyUFP4DUwJj5jNGrv3e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitin-khandagale/machine_learning_practice/blob/master/Tf_Idf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct4ZfO-B6clA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0xk9l0C-f2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "site_link = \"https://www.hopefully.live/post/catboost-vs-xgboost-credit-risk-calculation\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CByugmwf-lBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link_data = requests.get(site_link)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhkV95-8-sfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "soup = BeautifulSoup(link_data.content)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jWukTnu-xq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "para = soup.find_all('p')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1pmToRa-y5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = []\n",
        "\n",
        "for text in para:\n",
        "  data.append(text.get_text())"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su8QKIpMCMde",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "10c92c54-b13a-406d-eec9-2e08fbc5f7c4"
      },
      "source": [
        "data"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Updated: Jun 26',\n",
              " 'Catboost and XGBoost are the two titans of gradient boosting technique. Today we will be using German credit risk dataset from kaggle to compare the performance of these two. We also will be using GridSearchCV to tune the parameters to get the best out of them. You can download the file straight up from here',\n",
              " 'Lets import the necessary modules and read the file.',\n",
              " 'have closer look at our database',\n",
              " 'Lets split our data into training and testing sets. We could have cleaned missing values first before splitting but that would be easy task to our model to understand the output. We will split the data first then encode it separately .',\n",
              " 'Now that our data is separated, x_train and y_train is filled with categorical columns and missing data. We will use Target Encoding to get rid of these missing data and categorical features at once. Lets get all the categorical columns we have first.',\n",
              " 'Lets encode the categorical columns',\n",
              " 'Now we join the these encoded columns and remove original encoded columns',\n",
              " 'Lets have a look at our training and testing set.',\n",
              " 'We will be performing modeling on two algorithms using GridSearchCV. Lets define the function to make out task easier and in less code.',\n",
              " 'As defined, our function returns model accuracy with mean_absolute_error , rmse, mean_squared_error. Lets use this.',\n",
              " \"We got 73% accuracy with XGBoost using GridSearchCV . but we also got RMSE 0.51, MSE  and MAE are 0.26. Also we used minimum parameters for tuning here for example. You can use more parameters to boost the model further. Let's try CatBoost now\",\n",
              " 'CatBoost performed slightly better than XGBoost in this case. i also tried RandomForestClassiefier() and i got again better performance like 0.76%.  Share this if you find this helpful and if you have any suggestions do not forget to reach me using details at the bottom. You can also leave the message by filling form at the bottom of this page. Thank you.',\n",
              " 'You can find above code in my Github repository',\n",
              " '',\n",
              " '',\n",
              " 'Thanks for submitting!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBpZ8f7v_AbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l072OUNBmgA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "30aa1eb4-4a48-43e3-9fad-0abe481a4926"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyfreDyBBsQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOxKk9YQByPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sents = []\n",
        "\n",
        "for line in data:\n",
        "  sents.append(sent_tokenize(line))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvxlLlwLB9ot",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "a6e906db-92aa-42f5-9bc2-f425b0c86d63"
      },
      "source": [
        "sents"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Updated: Jun 26'],\n",
              " ['Catboost and XGBoost are the two titans of gradient boosting technique.',\n",
              "  'Today we will be using German credit risk dataset from kaggle to compare the performance of these two.',\n",
              "  'We also will be using GridSearchCV to tune the parameters to get the best out of them.',\n",
              "  'You can download the file straight up from here'],\n",
              " ['Lets import the necessary modules and read the file.'],\n",
              " ['have closer look at our database'],\n",
              " ['Lets split our data into training and testing sets.',\n",
              "  'We could have cleaned missing values first before splitting but that would be easy task to our model to understand the output.',\n",
              "  'We will split the data first then encode it separately .'],\n",
              " ['Now that our data is separated, x_train and y_train is filled with categorical columns and missing data.',\n",
              "  'We will use Target Encoding to get rid of these missing data and categorical features at once.',\n",
              "  'Lets get all the categorical columns we have first.'],\n",
              " ['Lets encode the categorical columns'],\n",
              " ['Now we join the these encoded columns and remove original encoded columns'],\n",
              " ['Lets have a look at our training and testing set.'],\n",
              " ['We will be performing modeling on two algorithms using GridSearchCV.',\n",
              "  'Lets define the function to make out task easier and in less code.'],\n",
              " ['As defined, our function returns model accuracy with mean_absolute_error , rmse, mean_squared_error.',\n",
              "  'Lets use this.'],\n",
              " ['We got 73% accuracy with XGBoost using GridSearchCV .',\n",
              "  'but we also got RMSE 0.51, MSE  and MAE are 0.26.',\n",
              "  'Also we used minimum parameters for tuning here for example.',\n",
              "  'You can use more parameters to boost the model further.',\n",
              "  \"Let's try CatBoost now\"],\n",
              " ['CatBoost performed slightly better than XGBoost in this case.',\n",
              "  'i also tried RandomForestClassiefier() and i got again better performance like 0.76%.',\n",
              "  'Share this if you find this helpful and if you have any suggestions do not forget to reach me using details at the bottom.',\n",
              "  'You can also leave the message by filling form at the bottom of this page.',\n",
              "  'Thank you.'],\n",
              " ['You can find above code in my Github repository'],\n",
              " [],\n",
              " [],\n",
              " ['Thanks for submitting!']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJF4YgDpB_Tx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = []"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgXKKGeQDQ1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for li in sents:\n",
        "  sentences = sentences + li"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXn4Tkn_DDCg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "49a812ae-b769-4640-8fcd-2c22f75078b7"
      },
      "source": [
        "sentences"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Updated: Jun 26',\n",
              " 'Catboost and XGBoost are the two titans of gradient boosting technique.',\n",
              " 'Today we will be using German credit risk dataset from kaggle to compare the performance of these two.',\n",
              " 'We also will be using GridSearchCV to tune the parameters to get the best out of them.',\n",
              " 'You can download the file straight up from here',\n",
              " 'Lets import the necessary modules and read the file.',\n",
              " 'have closer look at our database',\n",
              " 'Lets split our data into training and testing sets.',\n",
              " 'We could have cleaned missing values first before splitting but that would be easy task to our model to understand the output.',\n",
              " 'We will split the data first then encode it separately .',\n",
              " 'Now that our data is separated, x_train and y_train is filled with categorical columns and missing data.',\n",
              " 'We will use Target Encoding to get rid of these missing data and categorical features at once.',\n",
              " 'Lets get all the categorical columns we have first.',\n",
              " 'Lets encode the categorical columns',\n",
              " 'Now we join the these encoded columns and remove original encoded columns',\n",
              " 'Lets have a look at our training and testing set.',\n",
              " 'We will be performing modeling on two algorithms using GridSearchCV.',\n",
              " 'Lets define the function to make out task easier and in less code.',\n",
              " 'As defined, our function returns model accuracy with mean_absolute_error , rmse, mean_squared_error.',\n",
              " 'Lets use this.',\n",
              " 'We got 73% accuracy with XGBoost using GridSearchCV .',\n",
              " 'but we also got RMSE 0.51, MSE  and MAE are 0.26.',\n",
              " 'Also we used minimum parameters for tuning here for example.',\n",
              " 'You can use more parameters to boost the model further.',\n",
              " \"Let's try CatBoost now\",\n",
              " 'CatBoost performed slightly better than XGBoost in this case.',\n",
              " 'i also tried RandomForestClassiefier() and i got again better performance like 0.76%.',\n",
              " 'Share this if you find this helpful and if you have any suggestions do not forget to reach me using details at the bottom.',\n",
              " 'You can also leave the message by filling form at the bottom of this page.',\n",
              " 'Thank you.',\n",
              " 'You can find above code in my Github repository',\n",
              " 'Thanks for submitting!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPBR_TuKDKBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-apfjTm4EsPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfid = TfidfVectorizer()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJeFKOgeEu6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "response = tfid.fit_transform(sentences)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w52ECQvzEyaM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "69ca33d0-e752-40ff-8b76-14eebc5a9826"
      },
      "source": [
        "response.toarray()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.53408077, 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.29259855, 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.5569694 ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.24836027],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqzbg_hcE_dL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}